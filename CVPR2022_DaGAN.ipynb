{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ticklecatisback/My-colab-Collections/blob/main/CVPR2022_DaGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdO_RxQZLahB"
      },
      "source": [
        "# Deepfakes with the CVPR2022-DaGAN\n",
        "This notebook is a demo for Creating uncanny deepfakes \n",
        "* Github: https://github.com/harlanhong/CVPR2022-DaGAN.git\n",
        "* Paper: https://arxiv.org/abs/2203.06605"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCMFMJV7K-ag",
        "cellView": "form"
      },
      "source": [
        "#@title STEP1: Setup\n",
        "import os, urllib.request\n",
        "HOME = os.path.expanduser(\"~\")\n",
        "pathDoneCMD = f'{HOME}/doneCMD.sh'\n",
        "if not os.path.exists(f\"{HOME}/.ipython/ttmg.py\"):\n",
        "    hCode = \"https://raw.githubusercontent.com/sudo-ken/FFmpeg-for-GDrive/master/ttmg.py\"\n",
        "    urllib.request.urlretrieve(hCode, f\"{HOME}/.ipython/ttmg.py\")\n",
        " \n",
        "from ttmg import (\n",
        "    loadingAn,\n",
        "    textAn,\n",
        ")\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Installing Dependencies...\", ty='twg')\n",
        "\n",
        "!git clone https://github.com/harlanhong/CVPR2022-DaGAN.git\n",
        "%cd CVPR2022-DaGAN\n",
        "!pip install -r requirements.txt\n",
        "!pip install numba\n",
        "\n",
        "## Install the Face Alignment lib\n",
        "%cd face-alignment\n",
        "!pip install -r requirements.txt\n",
        "!python setup.py install\n",
        "!pip install imageio-ffmpeg\n",
        "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip3 install torch torchvision torchaudio\n",
        "!pip install opencv-python\n",
        "!pip install scipy>=0.17.0\n",
        "!pip install scikit-image\n",
        "!pip install numba\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "def showVideo(path):\n",
        "  mp4 = open(str(path),'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  return HTML(\"\"\"\n",
        "  <video width=700 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url)\n",
        "\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP2: Download Dagen model\n",
        "from zipfile import ZipFile\n",
        "print(\"Follow this link:\")\n",
        "print(\"https://drive.google.com/open?id=1QHqh6CJB5e9lG0LPz8EIHtEKDDBH4Q3T\")\n",
        "print(\"and click 'Add shortcut to Drive' (top right of the screen)\\n\")\n",
        "print(\"After that, follow the link below, select the account where you saved the pretrained model,\")\n",
        "print(\"click the 'Allow' button.\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "with ZipFile('/content/gdrive/MyDrive/CVPR22_DaGAN.zip', 'r') as zipOBj:\n",
        "  zipOBj.extractall('/content/CVPR2022-DaGAN/depth')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "keEq2wCjv8Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP3: Select a Image\n",
        "image_url = 'https://media.bizj.us/view/img/11965652/glennfukuda-4196c-pr*1500xx1819-2425-61-0.jpg' #@param {type:\"string\"}\n",
        "import shutil\n",
        "import cv2\n",
        "from IPython.display import Image\n",
        "\n",
        "if image_url:\n",
        "  !wget \"$image_url\" -O /content/animate.png\n",
        "\n",
        "Image('/content/animate.png')"
      ],
      "metadata": {
        "id": "LI8NbqMSO_uX",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP3+: Upload your Image\n",
        "import os\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import shutil\n",
        "from IPython.display import Image\n",
        "uploaded = files.upload()\n",
        "dst ='putin.png' \n",
        "os.rename(list(uploaded.keys())[0], dst)\n",
        "\n",
        "Image('/content/putin.png')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "a_VuZkZepQsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Delete old input Image file\n",
        "%rm /content/animate.png\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DgaZFFNXYl4D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP4: Select a Youtube Video\n",
        "video_url = 'https://www.youtube.com/watch?v=vAnWYLTdvfY' #@param {type:\"string\"}\n",
        "\n",
        "import shutil\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "from IPython.display import clear_output\n",
        "import youtube_dl\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Downloading Video...\", ty='twg')\n",
        "if video_url:\n",
        "  !rm -f /content//fake.mp4\n",
        "  !youtube-dl -f \"bestvideo[ext=mp4][vcodec!*=av01][height<=360]+bestaudio[ext=m4a]/mp4[height<=360][vcodec!*=av01]/mp4[vcodec!*=av01]/mp4\" \"$video_url\" --merge-output-format mp4 -o /content/fake.mp4\n",
        "\n",
        "# cut the video\n",
        "textAn(\"Trimming Video...\", ty='twg')\n",
        "\n",
        "start_seconds = 0 #@param {type:\"number\"}\n",
        "duration_seconds =  60#@param {type:\"number\"}\n",
        "start_seconds = max(start_seconds,0)\n",
        "duration_seconds = max(duration_seconds,0)\n",
        "\n",
        "if duration_seconds:\n",
        "  !mv /content/fakee.mp4 /content/full_video.mp4\n",
        "  !ffmpeg -ss $start_seconds -t $duration_seconds -i /content/full_video.mp4 -f mp4 /content/wav2lip-hq/fake.mp4 -y\n",
        "\n",
        "!rm -df youtube.mp4\n",
        "# download the youtube with the given ID\n",
        "\n",
        "#delete video.mp4 if already exits\n",
        "!rm -f /content/CVPR2022-DaGAN/fake.mp4\n",
        "\n",
        "\n",
        "#Preview trimmed video\n",
        "clear_output()\n",
        "print(\"Trimmed Video\")\n",
        "showVideo('/content/fake.mp4')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tjn_yB1PnuCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP4+: Upload your Video File\n",
        "%cd /content\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "dst ='fake.mp4' \n",
        "os.rename(list(uploaded.keys())[0], dst)\n",
        "\n",
        "loadingAn(name=\"lds\")\n",
        "textAn(\"Moving Video...\", ty='twg')\n",
        "\n",
        "import os, sys, re\n",
        "video_file_path = \"/content/fake.mp4\"\n",
        "\n",
        "output_file_path = re.search(\"^[\\/].+\\/\", video_file_path)\n",
        "output_file_path_raw = output_file_path.group(0)\n",
        "delsplit = re.search(\"\\/(?:.(?!\\/))+$\", video_file_path)\n",
        "filename = re.sub(\"^[\\/]\", \"\", delsplit.group(0))\n",
        "filename_raw = re.sub(\".{4}$\", \"\", filename)\n",
        "file_extension = re.search(\".{3}$\", filename)\n",
        "file_extension_raw = file_extension.group(0)\n",
        "\n",
        "os.environ['inputFile'] = video_file_path\n",
        "os.environ['outputPath'] = output_file_path_raw\n",
        "os.environ['fileName'] = filename_raw\n",
        "os.environ['fileExtension'] = file_extension_raw\n",
        "\n",
        "!ffmpeg -hide_banner -i \"$inputFile\" -c copy -strict -2 \"$outputPath\"/\"$fileName\".mp4\n",
        "\n",
        "PATH_TO_YOU_VIDEO = '/content/fake.mp4'\n",
        "import subprocess\n",
        "\n",
        "#@markdown ### Trim the video (start, end) seconds\n",
        "#@markdown <i>Don't want to trim ? put <b>'start'</b> = -1 and <b>'end'</b> = -1</i>\n",
        "start =  -1#@param {type:\"integer\"}\n",
        "end =  -1#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown <i>Note: the trimmed video must have face on all frames</i>\n",
        "\n",
        "# delete start end\n",
        "interval = end - start\n",
        "\n",
        "textAn(\"Triming Video...\", ty='twg')\n",
        "\n",
        "#delete if file already exists\n",
        "!rm -f '/content/CVPR2022-DaGAN/fake.mp4'\n",
        "\n",
        "if start < 0 or end < 0:\n",
        "  #convert the video to specif location\n",
        "  !ffmpeg -i '$PATH_TO_YOU_VIDEO' '/content/fake.mp4'\n",
        "else:\n",
        "  # cut the video\n",
        "  !ffmpeg -i '$PATH_TO_YOU_VIDEO' -ss {start} -t {interval} -async 1 '/content/fake.mp4'\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Preview trimmed video\n",
        "clear_output()\n",
        "print(\"Input Video\")\n",
        "showVideo('/content/fake.mp4')"
      ],
      "metadata": {
        "id": "eDIfcb8yPKVa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Delete old input Video file\n",
        "%rm /content/fake.mp4\n",
        "from IPython.display import clear_output \n",
        "clear_output()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SU4yjX9bvQmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title STEP5: Change file\n",
        "%%writefile /content/CVPR2022-DaGAN/demo.py\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import os, sys\n",
        "import yaml\n",
        "from argparse import ArgumentParser\n",
        "from tqdm import tqdm\n",
        "import modules.generator as GEN\n",
        "import imageio\n",
        "import numpy as np\n",
        "from skimage.transform import resize\n",
        "from skimage import img_as_ubyte\n",
        "import torch\n",
        "from sync_batchnorm import DataParallelWithCallback\n",
        "import depth\n",
        "from modules.keypoint_detector import KPDetector\n",
        "from animate import normalize_kp\n",
        "from scipy.spatial import ConvexHull\n",
        "from collections import OrderedDict\n",
        "import pdb\n",
        "import cv2\n",
        "if sys.version_info[0] < 3:\n",
        "    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n",
        "\n",
        "def load_checkpoints(config_path, checkpoint_path, cpu=False):\n",
        "\n",
        "    with open(config_path) as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    if opt.kp_num != -1:\n",
        "        config['model_params']['common_params']['num_kp'] = opt.kp_num\n",
        "    generator = getattr(GEN, opt.generator)(**config['model_params']['generator_params'],**config['model_params']['common_params'])\n",
        "    if not cpu:\n",
        "        generator.cuda()\n",
        "    config['model_params']['common_params']['num_channels'] = 4\n",
        "    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
        "                             **config['model_params']['common_params'])\n",
        "    if not cpu:\n",
        "        kp_detector.cuda()\n",
        "    if cpu:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint_path,map_location=\"cuda:0\")\n",
        "        \n",
        "    ckp_generator = OrderedDict((k.replace('module.',''),v) for k,v in checkpoint['generator'].items())\n",
        "    generator.load_state_dict(ckp_generator)\n",
        "    ckp_kp_detector = OrderedDict((k.replace('module.',''),v) for k,v in checkpoint['kp_detector'].items())\n",
        "    kp_detector.load_state_dict(ckp_kp_detector)\n",
        "    \n",
        "    if not cpu:\n",
        "        generator = DataParallelWithCallback(generator)\n",
        "        kp_detector = DataParallelWithCallback(kp_detector)\n",
        "\n",
        "    generator.eval()\n",
        "    kp_detector.eval()\n",
        "    \n",
        "    return generator, kp_detector\n",
        "\n",
        "\n",
        "def make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n",
        "    sources = []\n",
        "    drivings = []\n",
        "    with torch.no_grad():\n",
        "        predictions = []\n",
        "        depth_gray = []\n",
        "        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n",
        "        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n",
        "        if not cpu:\n",
        "            source = source.cuda()\n",
        "            driving = driving.cuda()\n",
        "        outputs = depth_decoder(depth_encoder(source))\n",
        "        depth_source = outputs[(\"disp\", 0)]\n",
        "\n",
        "        outputs = depth_decoder(depth_encoder(driving[:, :, 0]))\n",
        "        depth_driving = outputs[(\"disp\", 0)]\n",
        "        source_kp = torch.cat((source,depth_source),1)\n",
        "        driving_kp = torch.cat((driving[:, :, 0],depth_driving),1)\n",
        "       \n",
        "        kp_source = kp_detector(source_kp)\n",
        "        kp_driving_initial = kp_detector(driving_kp) \n",
        "\n",
        "        # kp_source = kp_detector(source)\n",
        "        # kp_driving_initial = kp_detector(driving[:, :, 0])\n",
        "\n",
        "        for frame_idx in tqdm(range(driving.shape[2])):\n",
        "            driving_frame = driving[:, :, frame_idx]\n",
        "\n",
        "            if not cpu:\n",
        "                driving_frame = driving_frame.cuda()\n",
        "            outputs = depth_decoder(depth_encoder(driving_frame))\n",
        "            depth_map = outputs[(\"disp\", 0)]\n",
        "\n",
        "            gray_driving = np.transpose(depth_map.data.cpu().numpy(), [0, 2, 3, 1])[0]\n",
        "            gray_driving = 1-gray_driving/np.max(gray_driving)\n",
        "\n",
        "            frame = torch.cat((driving_frame,depth_map),1)\n",
        "            kp_driving = kp_detector(frame)\n",
        "\n",
        "            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n",
        "                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n",
        "                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n",
        "            out = generator(source, kp_source=kp_source, kp_driving=kp_norm,source_depth = depth_source, driving_depth = depth_map)\n",
        "\n",
        "            drivings.append(np.transpose(driving_frame.data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "            sources.append(np.transpose(source.data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
        "            depth_gray.append(gray_driving)\n",
        "    return sources, drivings, predictions,depth_gray\n",
        "\n",
        "\n",
        "def find_best_frame(source, driving, cpu=False):\n",
        "    import face_alignment\n",
        "\n",
        "    def normalize_kp(kp):\n",
        "        kp = kp - kp.mean(axis=0, keepdims=True)\n",
        "        area = ConvexHull(kp[:, :2]).volume\n",
        "        area = np.sqrt(area)\n",
        "        kp[:, :2] = kp[:, :2] / area\n",
        "        return kp\n",
        "\n",
        "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=True,\n",
        "                                      device='cpu' if cpu else 'cuda')\n",
        "    kp_source = fa.get_landmarks(255 * source)[0]\n",
        "    kp_source = normalize_kp(kp_source)\n",
        "    norm  = float('inf')\n",
        "    frame_num = 0\n",
        "    for i, image in tqdm(enumerate(driving)):\n",
        "        kp_driving = fa.get_landmarks(255 * image)[0]\n",
        "        kp_driving = normalize_kp(kp_driving)\n",
        "        new_norm = (np.abs(kp_source - kp_driving) ** 2).sum()\n",
        "        if new_norm < norm:\n",
        "            norm = new_norm\n",
        "            frame_num = i\n",
        "    return frame_num\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument(\"--config\", required=True, help=\"path to config\")\n",
        "    parser.add_argument(\"--checkpoint\", default='vox-cpk.pth.tar', help=\"path to checkpoint to restore\")\n",
        "\n",
        "    parser.add_argument(\"--source_image\", default='sup-mat/source.png', help=\"path to source image\")\n",
        "    parser.add_argument(\"--driving_video\", default='sup-mat/source.png', help=\"path to driving video\")\n",
        "    parser.add_argument(\"--result_video\", default='result.mp4', help=\"path to output\")\n",
        "    \n",
        "    parser.add_argument(\"--relative\", dest=\"relative\", action=\"store_true\", help=\"use relative or absolute keypoint coordinates\")\n",
        "    parser.add_argument(\"--adapt_scale\", dest=\"adapt_scale\", action=\"store_true\", help=\"adapt movement scale based on convex hull of keypoints\")\n",
        "    parser.add_argument(\"--generator\", type=str, required=True)\n",
        "    parser.add_argument(\"--kp_num\", type=int, required=True)\n",
        "\n",
        "\n",
        "    parser.add_argument(\"--find_best_frame\", dest=\"find_best_frame\", action=\"store_true\", \n",
        "                        help=\"Generate from the frame that is the most alligned with source. (Only for faces, requires face_aligment lib)\")\n",
        "\n",
        "    parser.add_argument(\"--best_frame\", dest=\"best_frame\", type=int, default=None,  \n",
        "                        help=\"Set frame to start from.\")\n",
        " \n",
        "    parser.add_argument(\"--cpu\", dest=\"cpu\", action=\"store_true\", help=\"cpu mode.\")\n",
        " \n",
        "\n",
        "    parser.set_defaults(relative=False)\n",
        "    parser.set_defaults(adapt_scale=False)\n",
        "\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    depth_encoder = depth.ResnetEncoder(18, False)\n",
        "    depth_decoder = depth.DepthDecoder(num_ch_enc=depth_encoder.num_ch_enc, scales=range(4))\n",
        "    loaded_dict_enc = torch.load('depth/models/weights_19/encoder.pth')\n",
        "    loaded_dict_dec = torch.load('depth/models/weights_19/depth.pth')\n",
        "    filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in depth_encoder.state_dict()}\n",
        "    depth_encoder.load_state_dict(filtered_dict_enc)\n",
        "    depth_decoder.load_state_dict(loaded_dict_dec)\n",
        "    depth_encoder.eval()\n",
        "    depth_decoder.eval()\n",
        "    if not opt.cpu:\n",
        "        depth_encoder.cuda()\n",
        "        depth_decoder.cuda()\n",
        "\n",
        "    source_image = imageio.imread(opt.source_image)\n",
        "    reader = imageio.get_reader(opt.driving_video)\n",
        "    fps = reader.get_meta_data()['fps']\n",
        "    driving_video = []\n",
        "    try:\n",
        "        for im in reader:\n",
        "            driving_video.append(im)\n",
        "    except RuntimeError:\n",
        "        pass\n",
        "    reader.close()\n",
        "\n",
        "    source_image = resize(source_image, (256, 256))[..., :3]\n",
        "    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
        "    generator, kp_detector = load_checkpoints(config_path=opt.config, checkpoint_path=opt.checkpoint, cpu=opt.cpu)\n",
        "\n",
        "    if opt.find_best_frame or opt.best_frame is not None:\n",
        "        i = opt.best_frame if opt.best_frame is not None else find_best_frame(source_image, driving_video, cpu=opt.cpu)\n",
        "        print (\"Best frame: \" + str(i))\n",
        "        driving_forward = driving_video[i:]\n",
        "        driving_backward = driving_video[:(i+1)][::-1]\n",
        "        sources_forward, drivings_forward, predictions_forward,depth_forward = make_animation(source_image, driving_forward, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n",
        "        sources_backward, drivings_backward, predictions_backward,depth_backward = make_animation(source_image, driving_backward, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n",
        "        predictions = predictions_backward[::-1] + predictions_forward[1:]\n",
        "        sources = sources_backward[::-1] + sources_forward[1:]\n",
        "        drivings = drivings_backward[::-1] + drivings_forward[1:]\n",
        "        depth_gray = depth_backward[::-1] + depth_forward[1:]\n",
        "\n",
        "    else:\n",
        "        # predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n",
        "        sources, drivings, predictions,depth_gray = make_animation(source_image, driving_video, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n",
        "    imageio.mimsave('demo.mp4', [img_as_ubyte(p) for p in predictions], fps=fps)\n",
        "    imageio.mimsave(opt.result_video, [np.concatenate((img_as_ubyte(s),img_as_ubyte(d),img_as_ubyte(p)),1) for (s,d,p) in zip(sources, drivings, predictions)], fps=fps)\n",
        "    imageio.mimsave(\"gray.mp4\", depth_gray, fps=fps)\n",
        "    # merge the gray video\n",
        "    animation = np.array(imageio.mimread(opt.result_video,memtest=False))\n",
        "    gray = np.array(imageio.mimread(\"gray.mp4\",memtest=False))\n",
        "\n",
        "    src_dst = animation[:,:,:512,:]\n",
        "    animate = animation[:,:,512:,:]\n",
        "    merge = np.concatenate((src_dst,gray,animate),2)\n",
        "    imageio.mimsave(opt.result_video, merge, fps=fps)\n",
        "    #Transfer to gif\n",
        "    # from moviepy.editor import *\n",
        "    # clip = (VideoFileClip(opt.result_video))\n",
        "    # clip.write_gif(\"{}.gif\".format(opt.result_video))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wboZ7PX2OtLt",
        "outputId": "af4d5e34-1cf4-460f-a7de-5124a3beaad8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/CVPR2022-DaGAN/demo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxi6-riLOgnm",
        "cellView": "form"
      },
      "source": [
        "#@title STEP6: Animate it\n",
        "kp_num = 15#@param {type:\"integer\"}\n",
        "generator = \"DepthAwareGenerator\" #@param {type:\"string\"}\n",
        "import os\n",
        "%cd /content/CVPR2022-DaGAN\n",
        "import os.path\n",
        "from os import path\n",
        "import yaml\n",
        "\n",
        "!mkdir '/content/CVPR2022-DaGAN/depth/models/weights_19'\n",
        "!cp '/content/CVPR2022-DaGAN/depth/CVPR22_DaGAN/depth_face_model/encoder.pth' '/content/CVPR2022-DaGAN/depth/models/weights_19'\n",
        "!cp '/content/CVPR2022-DaGAN/depth/CVPR22_DaGAN/depth_face_model/depth.pth' '/content/CVPR2022-DaGAN/depth/models/weights_19'\n",
        "\n",
        "!cd /content/CVPR2022-DaGAN && python demo.py --config \"/content/CVPR2022-DaGAN/config/vox-adv-256.yaml\" --driving_video \"/content/fake.mp4\" --source_image \"/content/animate.png\" --checkpoint \"/content/CVPR2022-DaGAN/depth/CVPR22_DaGAN/DaGAN_vox_adv_256.pth.tar\" --relative --adapt_scale --kp_num $kp_num --generator $generator\n",
        "\n",
        "#Preview trimmed video\n",
        "clear_output()\n",
        "print(\"Trimmed Video\")\n",
        "showVideo('/content/CVPR2022-DaGAN/demo.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}